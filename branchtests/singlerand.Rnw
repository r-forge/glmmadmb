\documentclass{article}
\begin{document}
\newcommand{\code}[1]{{\tt #1 }}
\SweaveOpts{keep.source=TRUE}
<<echo=FALSE>>=
options(continue=" ")
@ 

\subsubsection{Tests:}

Data sets.

\begin{itemize}
  \item Set 1: Poisson-distributed data.
    Regression design ($\beta=\{1,2\}$), $x\sim U[0,1]$.
    10 blocks, 10 reps per block.  Random intercept with $\sigma=1$.
  \item Set 2: as above but random slope with $\sigma_s=0.5$.
  \item Set 3: \code{epil2} data set from \code{glmmADMB}
  \end{itemize}
  
Models.  
  \begin{itemize}
    \item 0: Data set 1: intercept-only model, random intercepts.
    \item 1: Data set 1: \verb!y~x!, random intercepts.
    \item 2: Data set 1: \verb!y~x!, random slopes.
    \item 3--5: as 0--2, but with data set 2
    \item 6: \code{epil2} data, \verb!y~Base*trt+Age+Visit!,
      random slopes (\code{Visit}) by subject,
      negative binomial
    \item 7: as 6, but Poisson (bad model, but useful for comparison
      with \code{lme4})
\end{itemize}
All models with multiple random effects (i.e. 2, 5, 6, 7) are fitted
with diagonal variance-covariance matrix (i.e. default
\code{corStruct="diag"} in glmmADMB, non-default \code{(1|group)+(0+x|group)}
in glmer).

Would like to test \code{cbpp} data from \code{lme4},
but it's binomial with $N>1$.

\section{Comparisons}

<<>>=
load("singlerand_batch.RData")
@ 

<<>>=
library(lme4) ## handy for glmer accessors
library(glmmADMB)
@ 

All fits OK, \emph{except}:

\begin{itemize}
  \item Warnings (\code{Estimated covariance matrix may not be positive definite})
    for models 0-5, 7 with old glmmADMB.  (Is this a real difference or a 
    reporting difference?  Doesn't seem to be in the TPL file, hence must (?)
    be coming from ADMB, hence must (?) be real \ldots)
  \item new glmmADMB fails on model 6
  \end{itemize}

<<sumfun,echo=FALSE>>=
sumfun <- function(o,n,m) {
  fixed <- cbind(coef(o),coef(n),fixef(m))
  ran <- cbind(o$U,n$U[[1]],ranef(m)[[1]])
  ransum <- apply(ran,2,function(x) c(min=min(x),mean=mean(x),max=max(x)))
  if (ncol(ransum)>3) { ## multiple REs
    dim(ransum) <- c(length(ransum)/3,3)
  }
  LL <- c(logLik(o),logLik(n),logLik(m))
  rv <- cbind(diag(o$S),diag(n$S[[1]]),sapply(VarCorr(m),c))
  mm <- rbind(fixed,rv,LL,ransum)
  rnames <- rownames(rv)
  dimnames(mm) <- list(c(names(fixef(m)),
                        paste("var(RE)",rnames,sep="."),
                        "logLik",
                        paste("U",
                              c(outer(c("min","mean","max"),rnames,
                                      function(x,y) paste(y,x,sep="."))),sep=".")),
                      c("glmmADMB(orig)","glmmADMB(new)","glmer"))
  mm
}

require(Hmisc)
sumfun2 <- function(o,n,m) {
  latex(sumfun(o,n,m),file="",dec=7,table.env=FALSE)
}
@ 

\subsubsection{Model 0}
<<m0,echo=FALSE,results=tex>>=
sumfun2(g0_old,g0_new,g0_lme4)
@ 

\textbf{Conclusions}: 
\begin{itemize}
  \item Precision of glmmADMB results is truncated, presumably
    from printing/reading intermediate files in lowered precision.
    This applies throughout: not a big deal, but would be nice 
    (and presumably not too hard) to fix.
  \item new glmmADMB and glmer agree on intercept, intercept variance
  \item new glmmADMB and old glmmADMB agree on random effects.
  \item Log-likelihood is obviously computed differently for glmer 
    and glmmADMB; new and old glmmADMB differ by a disturbing amount.
    (Don't know whether this is computation or poorer fit; could check
    this.)
  \end{itemize}
  
  
\subsubsection{Model 1}
<<m1,echo=FALSE,results=tex>>=
sumfun2(g1_old,g1_new,g1_lme4)
@ 
\textbf{Conclusions:}
\begin{itemize}
  \item All three agree on coefficients
  \item glmmADMBs agree very closely on RE var (although glmer is close),
    REs
  \end{itemize}

\subsubsection{Model 2}
<<m2,echo=FALSE,results=tex>>=
sumfun2(g2_old,g2_new,g2_lme4)
@ 
\textbf{Conclusions:}
\begin{itemize}
  \item All three agree on coefficients
  \item glmmADMBs agree very closely on RE var (although glmer is close),
    all three set var of $x$ (correctly) to zero
  \end{itemize}

\subsubsection{Model 3}
<<m3,echo=FALSE,results=tex>>=
sumfun2(g3_old,g3_new,g3_lme4)
@ 
\textbf{Conclusions:}
\begin{itemize}
  \item Like model 0: new glmmADMB agrees with glmer
    (is this those two getting stuck at a slightly suboptimal fit?)
  \item log-likelihoods are all over the map
  \end{itemize}

\subsubsection{Model 4}
<<m4,echo=FALSE,results=tex>>=
sumfun2(g4_old,g4_new,g4_lme4)
@ 
\textbf{Conclusions:}
\begin{itemize}
  \item Like model 1
  \end{itemize}
  
\subsubsection{Model 5}
<<m5,echo=FALSE,results=tex>>=
sumfun2(g5_old,g5_new,g5_lme4)
@ 
\textbf{Conclusions:}
\begin{itemize}
  \item Like model 2 (but now slope variances are non-zero)
  \end{itemize}

\subsubsection{Model 6}
  
glmer can't do this one (negative binomial),
and new glmmADMB fails.

  \subsubsection{Model 7}
<<echo=FALSE,results=tex>>=
sumfun2(g7_old,g7_new,g7_lme4)
@ 

<<echo=FALSE,eval=FALSE>>=
summary(ranef(g7_new)$subject[,1]*sqrt(g7_new$S$subject[1,1]))
summary(ranef(g7_lme4)$subject[,1])
object <- g5_new
sweep(object$U[[1]],sqrt(diag(object$S[[1]])),MARGIN=2,FUN="*")
mapply(sweep,object$U,sqrt(diag(object$S[[1]])),MARGIN=2,FUN="*")
@ 
\end{document}
