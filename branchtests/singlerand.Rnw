\documentclass{article}
\begin{document}
\newcommand{\code}[1]{{\tt #1 }}
\SweaveOpts{keep.source=TRUE}
<<echo=FALSE>>=
options(continue=" ")
@ 

\section{Tests}

\subsection{Data sets}

\begin{itemize}
  \item Set 1: Poisson-distributed data.
    Regression design ($\beta=\{1,2\}$), $x\sim U[0,1]$.
    10 blocks, 10 reps per block.  Random intercept with $\sigma=1$.
  \item Set 2: as above but random slope with $\sigma_s=0.5$.
  \item Set 3: \code{epil2} data set from \code{glmmADMB}
  \end{itemize}
  
\subsection{Models}
  \begin{itemize}
    \item 0: Data set 1: intercept-only model, random intercepts.
    \item 1: Data set 1: \verb!y~x!, random intercepts.
    \item 2: Data set 1: \verb!y~x!, random slopes.
    \item 3--5: as 0--2, but with data set 2
    \item 6: \code{epil2} data, \verb!y~Base*trt+Age+Visit!,
      random slopes (\code{Visit}) by subject,
      negative binomial
    \item 7: as 6, but Poisson (bad model, but useful for comparison
      with \code{lme4})
\end{itemize}

All models with multiple random effects (i.e. 2, 5, 6, 7) are fitted
with diagonal variance-covariance matrix (i.e. default
\code{corStruct="diag"} in glmmADMB, non-default \code{(1|group)+(0+x|group)}
in glmer).

Note that random effects as stored in \code{glmm.admb} objects are
unscaled; those in \code{mer} are scaled by the standard deviation
of the relevant random effect.

Would like to test \code{cbpp} data from \code{lme4},
but it's binomial with $N>1$ so not currently possible with glmmADMB.

\section{Comparisons}

<<>>=
load("singlerand_batch.RData")
@ 

<<>>=
library(glmmADMB) ## MUST load this first!
library(lme4) ## handy for glmer accessors
@ 

All fits OK, \emph{except}:

\begin{itemize}
  \item Warnings (\code{Estimated covariance matrix may not be positive definite})
    for models 0-5, 7 with old glmmADMB.  (Is this a real difference or a 
    reporting difference?  Doesn't seem to be in the TPL file, hence must (?)
    be coming from ADMB, hence must (?) be real \ldots)
  \item new glmmADMB fails on model 6
  \end{itemize}

<<sumfun,echo=FALSE>>=
sumfun <- function(o,n,m,t.o,t.n,t.m) {
  fixed <- cbind(coef(o),coef(n),fixef(m))
  ran <- cbind(sweep(o$U,2,sqrt(diag(o$S)),"*"),
               ranef(n)[[1]],ranef(m)[[1]])
  ransum <- apply(ran,2,function(x) c(min=min(x),mean=mean(x),max=max(x)))
  if (ncol(ransum)>3) { ## multiple REs
    dim(ransum) <- c(length(ransum)/3,3)
  }
  LL <- c(logLik(o),logLik(n),logLik(m))
  rv <- cbind(diag(o$S),diag(n$S[[1]]),sapply(VarCorr(m),c))
  times <- round(c(t.o[3],t.n[3],t.m[3]),2)
  mm <- rbind(fixed,rv,LL,ransum,times)
  rnames <- rownames(rv)
  dimnames(mm) <- list(c(names(fixef(m)),
                        paste("var(RE)",rnames,sep="."),
                        "logLik",
                        paste("U",
                              c(outer(c("min","mean","max"),rnames,
                                      function(x,y) paste(y,x,sep="."))),sep="."),
                         "time"),
                      c("glmmADMB(orig)","glmmADMB(new)","glmer"))
  mm
}

require(Hmisc)
sumfun2 <- function(o,n,m,t.o,t.n,t.m) {
  latex(sumfun(o,n,m,t.o,t.n,t.m),file="",dec=7,table.env=FALSE)
}
@ 

\subsubsection{Model 0}
<<m0,echo=FALSE,results=tex>>=
sumfun2(g0_old,g0_new,g0_lme4,t0_old,t0_new,t0_lme4)
@ 

\textbf{Conclusions}: 
\begin{itemize}
  \item Precision of glmmADMB results is truncated, presumably
    from printing/reading intermediate files in lowered precision.
    This applies throughout: not a big deal, but would be nice 
    (and presumably not too hard) to fix.
  \item new glmmADMB and glmer agree on intercept, intercept variance, random effects
  \item Log-likelihood is obviously computed differently for glmer 
    and glmmADMB; new and old glmmADMB differ by a disturbing amount.
    (Don't know whether this is computation or poorer fit; could check
    this.)
  \end{itemize}
  
  
\subsubsection{Model 1}
<<m1,echo=FALSE,results=tex>>=
sumfun2(g1_old,g1_new,g1_lme4,t1_old,t1_new,t1_lme4)
@ 
\textbf{Conclusions:}
\begin{itemize}
  \item All three agree on coefficients, REs
  \item glmmADMBs agree very closely on RE var (although glmer is pretty close too)
  \end{itemize}

\subsubsection{Model 2}
<<m2,echo=FALSE,results=tex>>=
sumfun2(g2_old,g2_new,g2_lme4,t2_old,t2_new,t2_lme4)
@ 
\textbf{Conclusions:}
\begin{itemize}
  \item Like model 2: all agree closely
  \end{itemize}

\subsubsection{Model 3}
<<m3,echo=FALSE,results=tex>>=
sumfun2(g3_old,g3_new,g3_lme4,t3_old,t3_new,t3_lme4)
@ 
\textbf{Conclusions:}
\begin{itemize}
  \item Like model 0: new glmmADMB agrees with glmer
    (is this those two getting stuck at a slightly suboptimal fit?)
  \item log-likelihoods are all over the map
  \end{itemize}

\subsubsection{Model 4}
<<m4,echo=FALSE,results=tex>>=
sumfun2(g4_old,g4_new,g4_lme4,t4_old,t4_new,t4_lme4)
@ 
\textbf{Conclusions:}
\begin{itemize}
  \item Like model 1
  \end{itemize}
  
\subsubsection{Model 5}
<<m5,echo=FALSE,results=tex>>=
sumfun2(g5_old,g5_new,g5_lme4,t5_old,t5_new,t5_lme4)
@ 
\textbf{Conclusions:}
\begin{itemize}
  \item Like model 2 (but now slope variances are non-zero)
  \end{itemize}

\subsubsection{Model 6}
  
glmer can't do this one (negative binomial),
and new glmmADMB fails.

  \subsubsection{Model 7}
<<m6,echo=FALSE,results=tex>>=
sumfun2(g7_old,g7_new,g7_lme4,t7_old,t7_new,t7_lme4)
@ 
\textbf{Conclusions:}
\begin{itemize}
  \item As before: glmer and new glmmADMB agree
  \end{itemize}


\section{To do}
\begin{itemize}
\item Try to figure out if glmer/new glmmADMB are getting stuck
  (e.g. evaluate likelihoods from scratch at both sets of parameter
  values, or evaluate likelihoods from each set in the other package/version
\item incorporate timings  
\item incorporate fits from glmmML (can do intercept-only RE Poisson/binomial
  models, with Laplace [runs done, just have to do reporting]
\end{itemize}
  
  
\end{document}
